{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8j2wfe14KOA"
      },
      "outputs": [],
      "source": [
        "! pip install evaluate\n",
        "! pip install timm\n",
        "! pip install torch torchvision transformers datasets\n",
        "! pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRb4WBGF4KOC",
        "outputId": "3f8cf7e1-46ad-48f9-ea5d-7e7001e5be70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove '__MACOSX': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! rm -r __MACOSX dataset dataset.zip\n",
        "! cp -r /content/drive/MyDrive/dataset.zip /content\n",
        "! unzip /content/dataset.zip\n",
        "! rm -r __MACOSX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F47GCERN4KOC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class myDataset(Dataset):\n",
        "    def __init__(self, root, annotation, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.coco = COCO(annotation)\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        coco = self.coco\n",
        "        \n",
        "        img_id = self.ids[index]\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
        "        coco_annotation = coco.loadAnns(ann_ids)\n",
        "        path = coco.loadImgs(img_id)[0]['file_name']\n",
        "        img = Image.open(os.path.join(self.root, path))\n",
        "        num_objs = len(coco_annotation)\n",
        "\n",
        "        # In coco format, bbox = [xmin, ymin, width, height]\n",
        "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            xmin = coco_annotation[i]['bbox'][0]\n",
        "            ymin = coco_annotation[i]['bbox'][1]\n",
        "            xmax = xmin + coco_annotation[i]['bbox'][2]\n",
        "            ymax = ymin + coco_annotation[i]['bbox'][3]\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32) #???\n",
        "\n",
        "        labels = []\n",
        "        for i in range(num_objs):\n",
        "            labels.append(coco_annotation[i]['category_id'])\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        img_id = torch.tensor([img_id])\n",
        "\n",
        "        areas = []\n",
        "        for i in range(num_objs):\n",
        "            areas.append(coco_annotation[i]['area'])\n",
        "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
        "\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        my_annotation = {}\n",
        "        my_annotation[\"boxes\"] = boxes\n",
        "        my_annotation[\"labels\"] = labels\n",
        "        my_annotation[\"image_id\"] = img_id\n",
        "        my_annotation[\"area\"] = areas\n",
        "        my_annotation[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, my_annotation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U9UHQJLF4KOD"
      },
      "outputs": [],
      "source": [
        "def get_transform():\n",
        "    custom_transforms = []\n",
        "    custom_transforms.append(torchvision.transforms.ToTensor())\n",
        "    return torchvision.transforms.Compose(custom_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOnEVnhR4KOD",
        "outputId": "8c082119-26a1-41b5-a02a-4d87fbd94400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "# path to dataset and coco file\n",
        "data_dir = 'dataset'\n",
        "train_coco = 'dataset/coco_train.json'\n",
        "\n",
        "# create Dataset\n",
        "train_dataset = myDataset(root=data_dir,\n",
        "                          annotation=train_coco,\n",
        "                          transforms=get_transform()\n",
        "                          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MP7QDEva6ffu"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "train_batch_size = 4\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                          batch_size=train_batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp7es_vQ4KOD",
        "outputId": "371c6eea-4b5d-493d-defb-4a4888670f87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 1/42, Loss: 1.1098710298538208\n",
            "Iteration: 2/42, Loss: 0.5174397826194763\n",
            "Iteration: 3/42, Loss: 0.32265380024909973\n",
            "Iteration: 4/42, Loss: 0.3868410885334015\n",
            "Iteration: 5/42, Loss: 0.31667208671569824\n",
            "Iteration: 6/42, Loss: 0.3291205167770386\n",
            "Iteration: 7/42, Loss: 0.32308775186538696\n",
            "Iteration: 8/42, Loss: 0.3167784512042999\n",
            "Iteration: 9/42, Loss: 0.28030771017074585\n",
            "Iteration: 10/42, Loss: 0.20892705023288727\n",
            "Iteration: 11/42, Loss: 0.22345903515815735\n",
            "Iteration: 12/42, Loss: 0.199838787317276\n",
            "Iteration: 13/42, Loss: 0.19196352362632751\n",
            "Iteration: 14/42, Loss: 0.22355619072914124\n",
            "Iteration: 15/42, Loss: 0.2158464640378952\n",
            "Iteration: 16/42, Loss: 0.20521707832813263\n",
            "Iteration: 17/42, Loss: 0.16400782763957977\n",
            "Iteration: 18/42, Loss: 0.19538792967796326\n",
            "Iteration: 19/42, Loss: 0.22171476483345032\n",
            "Iteration: 20/42, Loss: 0.16077221930027008\n",
            "Iteration: 21/42, Loss: 0.22037586569786072\n",
            "Iteration: 22/42, Loss: 0.1863775998353958\n",
            "Iteration: 23/42, Loss: 0.17574751377105713\n",
            "Iteration: 24/42, Loss: 0.20783497393131256\n",
            "Iteration: 25/42, Loss: 0.2072809338569641\n",
            "Iteration: 26/42, Loss: 0.1885705143213272\n",
            "Iteration: 27/42, Loss: 0.1770251840353012\n",
            "Iteration: 28/42, Loss: 0.15649332106113434\n",
            "Iteration: 29/42, Loss: 0.1688840687274933\n",
            "Iteration: 30/42, Loss: 0.15328538417816162\n",
            "Iteration: 31/42, Loss: 0.12665808200836182\n",
            "Iteration: 32/42, Loss: 0.15585611760616302\n",
            "Iteration: 33/42, Loss: 0.14123296737670898\n",
            "Iteration: 34/42, Loss: 0.12338482588529587\n",
            "Iteration: 35/42, Loss: 0.11018151044845581\n",
            "Iteration: 36/42, Loss: 0.10015609860420227\n",
            "Iteration: 37/42, Loss: 0.10533355176448822\n",
            "Iteration: 38/42, Loss: 0.09377074241638184\n",
            "Iteration: 39/42, Loss: 0.09643224626779556\n",
            "Iteration: 40/42, Loss: 0.11877475678920746\n",
            "Iteration: 41/42, Loss: 0.09836244583129883\n",
            "Iteration: 42/42, Loss: 0.06787795573472977\n",
            "Iteration: 1/42, Loss: 0.07947567850351334\n",
            "Iteration: 2/42, Loss: 0.08686047047376633\n",
            "Iteration: 3/42, Loss: 0.07445669919252396\n",
            "Iteration: 4/42, Loss: 0.08412128686904907\n",
            "Iteration: 5/42, Loss: 0.06795831024646759\n",
            "Iteration: 6/42, Loss: 0.07075504213571548\n",
            "Iteration: 7/42, Loss: 0.07081212103366852\n",
            "Iteration: 8/42, Loss: 0.0647001564502716\n",
            "Iteration: 9/42, Loss: 0.05738713592290878\n",
            "Iteration: 10/42, Loss: 0.07997474819421768\n",
            "Iteration: 11/42, Loss: 0.06526104360818863\n",
            "Iteration: 12/42, Loss: 0.054479967802762985\n",
            "Iteration: 13/42, Loss: 0.041753191500902176\n",
            "Iteration: 14/42, Loss: 0.04904445633292198\n",
            "Iteration: 15/42, Loss: 0.060478635132312775\n",
            "Iteration: 16/42, Loss: 0.0458340160548687\n",
            "Iteration: 17/42, Loss: 0.048194702714681625\n",
            "Iteration: 18/42, Loss: 0.058181412518024445\n",
            "Iteration: 19/42, Loss: 0.053316351026296616\n",
            "Iteration: 20/42, Loss: 0.05875717103481293\n",
            "Iteration: 21/42, Loss: 0.06103009358048439\n",
            "Iteration: 22/42, Loss: 0.04903935641050339\n",
            "Iteration: 23/42, Loss: 0.03613436967134476\n",
            "Iteration: 24/42, Loss: 0.037833381444215775\n",
            "Iteration: 25/42, Loss: 0.0538603849709034\n",
            "Iteration: 26/42, Loss: 0.06106574833393097\n",
            "Iteration: 27/42, Loss: 0.05127258971333504\n",
            "Iteration: 28/42, Loss: 0.038279108703136444\n",
            "Iteration: 29/42, Loss: 0.042004600167274475\n",
            "Iteration: 30/42, Loss: 0.042313288897275925\n",
            "Iteration: 31/42, Loss: 0.03535215184092522\n",
            "Iteration: 32/42, Loss: 0.054201602935791016\n",
            "Iteration: 33/42, Loss: 0.03207223862409592\n",
            "Iteration: 34/42, Loss: 0.034648846834897995\n",
            "Iteration: 35/42, Loss: 0.04311042279005051\n",
            "Iteration: 36/42, Loss: 0.04110966622829437\n",
            "Iteration: 37/42, Loss: 0.04164860025048256\n",
            "Iteration: 38/42, Loss: 0.03264675661921501\n",
            "Iteration: 39/42, Loss: 0.035876642912626266\n",
            "Iteration: 40/42, Loss: 0.02985353209078312\n",
            "Iteration: 41/42, Loss: 0.043846458196640015\n",
            "Iteration: 42/42, Loss: 0.04992898181080818\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    \n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "    \n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# classes + background\n",
        "num_classes = 3\n",
        "\n",
        "num_epochs = 2\n",
        "model = get_model_instance_segmentation(num_classes)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# parameters\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "len_dataloader = len(data_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    i = 0\n",
        "    for imgs, annotations in data_loader:\n",
        "        i += 1\n",
        "        imgs = list(img.to(device) for img in imgs)\n",
        "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
        "        loss_dict = model(imgs, annotations)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uxLIdbdp4KOE"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "torch.save(model.state_dict(), 'faster_rcnn_model.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
